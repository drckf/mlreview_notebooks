{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCPB 20-21 exercise 2\n",
    "\n",
    "### Saverio Monaco\n",
    "\n",
    "### Gerardo Carmona\n",
    "\n",
    "### Hilario Capettini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"DATA/sequences16.csv\"\n",
    "sx, sy = np.loadtxt(fname,delimiter=',',\n",
    "                   usecols=(0,1), unpack=True, dtype=str)\n",
    "N = len(sy)\n",
    "\n",
    "Ls = len(sx[0])\n",
    "\n",
    "\n",
    "perc_train = 0.8 # I kept the same train-test ratio as in the lesson\n",
    "N_train = int(N*perc_train)\n",
    "N_test = N - N_train\n",
    "\n",
    "print('Size of the sequences:   ',Ls)\n",
    "print('Size of the data:        ',N)\n",
    "print('Size of the training set:',N_train)\n",
    "print('Size of the test set:    ',N_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just printing some datas to check it\n",
    "# I noticed this isn't the same rule in the lessons, so it is unkown\n",
    "for _ in range(10):\n",
    "    print(sx[_],sy[_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the label set to int values:\n",
    "print(sy[0],type(sy[0]))\n",
    "y = sy.astype(int)\n",
    "print(y[0],type(y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Letters and number of letters\n",
    "Q = ['A', 'C', 'G', 'T']\n",
    "Nc = 4\n",
    "\n",
    "# building the dictionary\n",
    "onehc = {Q[i]: i for i in range (Nc)}\n",
    "\n",
    "L = Ls * Nc # lenght of the input array (64)\n",
    "\n",
    "# The following cycle will transform the input vectors (for ex.: AAGGTCTGCCGGCCGA) in a\n",
    "# binary like way\n",
    "#\n",
    "# A = 1000\n",
    "# C = 0100\n",
    "# G = 0010\n",
    "# T = 0001\n",
    "#\n",
    "#   A    A    G    G    T    C   ...\n",
    "# 1000 1000 0010 0010 0001 0100\n",
    "x = np.zeros((N,L))\n",
    "for n in range(N): #for all the samples\n",
    "    for i in range(Ls): # for every character\n",
    "        x[n][i*4 + onehc[sx[n][i]]] = 1\n",
    "        \n",
    "print(sx[0])\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train) = (x[:N_train],y[:N_train])\n",
    "(x_test, y_test) = (x[N_train:],y[N_train:])\n",
    "\n",
    "# Check the fraction of datas equal to one\n",
    "print(y_train.sum() / N_train)\n",
    "print(y_test.sum() / N_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Is the model converging with a smaller database of samples with longer sequences? By converging we mean reducing significantly the validation loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "model = Sequential() #start of implementing a neural network in keras\n",
    "\n",
    "#  x\n",
    "#  x\n",
    "#  x     h\n",
    "#  x     h\n",
    "#  x     h     h    o (0,1)\n",
    "#  x     h\n",
    "#  x\n",
    "#  x\n",
    "#             drop\n",
    "\n",
    "# FIRST LAYER (INPUT)\n",
    "model.add(Dense(L, input_shape=(L,),activation='relu')) # Dense stands for a normal layer\n",
    "\n",
    "# SECOND LAYER (HIDDEN 1)\n",
    "model.add(Dense(L/2,activation='relu'))\n",
    "\n",
    "# THIRD LAYER (HIDDEN 2)\n",
    "model.add(Dense(L/4,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# FORTH LAYER (OUTPUT) introducing dropouts\n",
    "model.add(Dense(1,activation='sigmoid')) #we need an activation function from 0 to 1\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "opt1 = SGD(learning_rate=0.01, momentum=0.9, nesterov= True)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=opt1,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "fit = model.fit(x_train, y_train,\n",
    "                epochs = 60, batch_size = 50,\n",
    "                validation_data = (x_test,y_test),\n",
    "                shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obs in ('accuracy', 'loss'):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(fit.history[obs], 'r', label = obs + ' of training data')\n",
    "    plt.plot(fit.history['val_'+obs], 'b--', label = obs + ' of test data')\n",
    "    plt.ylabel(obs)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Try to improve the performance of the DNN over the validation data set by “augmenting”\n",
    "the training data: For every sample there are L s -1 periodic shifts of the kind\n",
    "AAACCCTTTGGG→ GAAACCCTTTGG → GGAAACCCTTTG→ etc.\n",
    "We know that they can break the keys and provide a sample x’[n] with wrong label y[n]\n",
    "(which is the label of original sample x[n]), but they also enlarge the number of good\n",
    "samples for the DNN. Which of the two effects is prevalent?\n",
    "Is the situation improving by augmenting the training data from N t real samples to L s *N t ones with this procedure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implement a “grid search” as shown in NB11 to improve one or more of the aspects or\n",
    "parameters of the model. Possible tests include: different activation units (sigmoid, relu, elu,\n",
    "etc.), different minimization algorithms (ADAM, RMSprop, Nesterov, etc.), different\n",
    "dropouts, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. See if any rescaling of data may improve the results. For instance one may use [-0.5,+0.5]\n",
    "instead of [0,1] for every bit of x[n]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
