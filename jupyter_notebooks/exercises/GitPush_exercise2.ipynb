{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCPB 20-21 exercise 2\n",
    "\n",
    "### Saverio Monaco\n",
    "\n",
    "### Gerardo Carmona\n",
    "\n",
    "### Hilario Capettini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"DATA/sequences16.csv\"\n",
    "sx, sy = np.loadtxt(fname,delimiter=',',\n",
    "                   usecols=(0,1), unpack=True, dtype=str)\n",
    "N = len(sy)\n",
    "\n",
    "Ls = len(sx[0])\n",
    "\n",
    "\n",
    "perc_train = 0.8 # I kept the same train-test ratio as in the lesson\n",
    "N_train = int(N*perc_train)\n",
    "N_test = N - N_train\n",
    "\n",
    "print('Size of the sequences:   ',Ls)\n",
    "print('Size of the data:        ',N)\n",
    "print('Size of the training set:',N_train)\n",
    "print('Size of the test set:    ',N_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just printing some datas to check it\n",
    "# I noticed this isn't the same rule in the lessons, so it is unkown\n",
    "for _ in range(10):\n",
    "    print(sx[_],sy[_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converting the label set to int values:\n",
    "print(sy[0],type(sy[0]))\n",
    "y = sy.astype(int)\n",
    "print(y[0],type(y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Letters and number of letters\n",
    "Q = ['A', 'C', 'G', 'T']\n",
    "Nc = 4\n",
    "\n",
    "# building the dictionary\n",
    "onehc = {Q[i]: i for i in range (Nc)}\n",
    "\n",
    "L = Ls * Nc # lenght of the input array (64)\n",
    "\n",
    "# The following cycle will transform the input vectors (for ex.: AAGGTCTGCCGGCCGA) in a\n",
    "# binary like way\n",
    "#\n",
    "# A = 1000\n",
    "# C = 0100\n",
    "# G = 0010\n",
    "# T = 0001\n",
    "#\n",
    "#   A    A    G    G    T    C   ...\n",
    "# 1000 1000 0010 0010 0001 0100\n",
    "x = np.zeros((N,L))\n",
    "for n in range(N): #for all the samples\n",
    "    for i in range(Ls): # for every character\n",
    "        x[n][i*4 + onehc[sx[n][i]]] = 1\n",
    "        \n",
    "print(sx[0])\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train) = (x[:N_train],y[:N_train])\n",
    "(x_test, y_test) = (x[N_train:],y[N_train:])\n",
    "\n",
    "# Check the fraction of datas equal to one\n",
    "print(y_train.sum() / N_train)\n",
    "print(y_test.sum() / N_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Is the model converging with a smaller database of samples with longer sequences? By converging we mean reducing significantly the validation loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I MADE THE SAME TRAINING IMPLEMENTING 3 FUNCTIONS\n",
    "# layer_i is an array representing a hidden layer used to create a neural network:\n",
    "# layer_i = [nodes, activation, dropout_ratio]\n",
    "# hidden_layers is a array of the hidden layers\n",
    "layer_1 = [L/2, 'relu', 0]\n",
    "layer_2 = [L/4, 'relu', .2]\n",
    "hidden_layers = [layer_1,layer_2]\n",
    "\n",
    "# optimization chosen\n",
    "opt = SGD(learning_rate=0.01, momentum=0.9, nesterov= True)\n",
    "\n",
    "# the first function creates the Neural Network, it needs the input dimension and the array of layers\n",
    "def createmodel(input_dim, hidden_layers):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # we add the first layer (input layer)\n",
    "    model.add(Dense(input_dim, input_shape=(input_dim,)))\n",
    "    \n",
    "    # we add the hidden layers\n",
    "    for layer in hidden_layers:\n",
    "        model.add(Dense(layer[0],activation=layer[1]))\n",
    "        if layer[2]:\n",
    "            model.add(Dropout(layer[2]))\n",
    "    \n",
    "    # we add the output layer\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "# the second function trains the model created with createmodel, as a input it needs the output of createmodels\n",
    "# the optimization and the arguments of model.fit() (except for shuffle)\n",
    "def trainmodel(model, opt, *args, **kwargs):\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    fit = model.fit(*args, **kwargs, shuffle = True)\n",
    "    '''fit = model.fit(x_train, y_train,\n",
    "                    epochs = 60, batch_size = 50,\n",
    "                    validation_data = (x_test,y_test),\n",
    "                    shuffle = True)'''\n",
    "    \n",
    "    return fit\n",
    "\n",
    "# the third function takes the output of trainmodel and plot the loss of training data and test data\n",
    "def plotmodel(fit):\n",
    "    for obs in ('accuracy', 'loss'):\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(fit.history[obs], 'r', label = obs + ' of training data')\n",
    "        plt.plot(fit.history['val_'+obs], 'b--', label = obs + ' of test data')\n",
    "        plt.ylabel(obs)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define the model\n",
    "model = createmodel(L,hidden_layers)\n",
    "\n",
    "#Train the NN\n",
    "training = trainmodel(model, opt, x_train, y_train,\n",
    "          epochs=60, batch_size=50, validation_data= (x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "plotmodel(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Try to improve the performance of the DNN over the validation data set by “augmenting”\n",
    "the training data: For every sample there are L s -1 periodic shifts of the kind\n",
    "AAACCCTTTGGG→ GAAACCCTTTGG → GGAAACCCTTTG→ etc.\n",
    "We know that they can break the keys and provide a sample x’[n] with wrong label y[n]\n",
    "(which is the label of original sample x[n]), but they also enlarge the number of good\n",
    "samples for the DNN. Which of the two effects is prevalent?\n",
    "Is the situation improving by augmenting the training data from N t real samples to L s *N t ones with this procedure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function produces the permutations\n",
    "rot = lambda A: [A[i:]+A[:i] for i in range(len(A))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generate the augmented data\n",
    "sxx = []\n",
    "syy = np.zeros(len(sx)*Ls)\n",
    "for i in range(len(sx)):\n",
    "    sxx= np.append(sxx,rot(sx[i]))\n",
    "    syy[i*Ls:(i+1)*Ls] = sy[i]\n",
    "syy = syy.astype(int)\n",
    "\n",
    "#Now we permutate the samples \n",
    "permutation = np.random.permutation(sxx.shape[0])\n",
    "\n",
    "sxx = sxx[permutation] \n",
    "syy = syy[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(syy)\n",
    "\n",
    "Ls = len(sxx[0])\n",
    "\n",
    "perc_train = 0.8 # I kept the same train-test ratio as in the lesson\n",
    "N_train = int(N*perc_train)\n",
    "N_test = N - N_train\n",
    "\n",
    "print('Size of the sequences:   ',Ls)\n",
    "print('Size of the data:        ',N)\n",
    "print('Size of the training set:',N_train)\n",
    "print('Size of the test set:    ',N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = syy\n",
    "x = np.zeros((N,L))\n",
    "for n in range(N): #for all the samples\n",
    "    for i in range(Ls): # for every character\n",
    "        x[n][i*4 + onehc[sxx[n][i]]] = 1\n",
    "        \n",
    "print(sxx[0])\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train) = (x[:N_train],y[:N_train])\n",
    "(x_test, y_test) = (x[N_train:],y[N_train:])\n",
    "\n",
    "# Check the fraction of datas equal to one\n",
    "print(y_train.sum() / N_train)\n",
    "print(y_test.sum() / N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define the model\n",
    "model = createmodel(L,hidden_layers)\n",
    "\n",
    "#Train the NN\n",
    "training = trainmodel(model, opt, x_train, y_train,\n",
    "          epochs=60, batch_size=50, validation_data= (x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "plotmodel(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "plotmodel(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implement a “grid search” as shown in NB11 to improve one or more of the aspects or\n",
    "parameters of the model. Possible tests include: different activation units (sigmoid, relu, elu,\n",
    "etc.), different minimization algorithms (ADAM, RMSprop, Nesterov, etc.), different\n",
    "dropouts, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How should we measure it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. See if any rescaling of data may improve the results. For instance one may use [-0.5,+0.5]\n",
    "instead of [0,1] for every bit of x[n]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
