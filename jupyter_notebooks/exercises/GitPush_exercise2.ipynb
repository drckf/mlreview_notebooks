{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCPB 20-21 exercise 2\n",
    "\n",
    "### Saverio Monaco\n",
    "\n",
    "### Gerardo Carmona\n",
    "\n",
    "### Hilario Capettini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import time\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import Adadelta, Adagrad, Adam, Adamax, RMSprop, SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"DATA/sequences16.csv\"\n",
    "sx, sy = np.loadtxt(fname,delimiter=',',\n",
    "                   usecols=(0,1), unpack=True, dtype=str)\n",
    "N = len(sy)\n",
    "\n",
    "Ls = len(sx[0])\n",
    "\n",
    "\n",
    "perc_train = 0.8 # I kept the same train-test ratio as in the lesson\n",
    "N_train = int(N*perc_train)\n",
    "N_test = N - N_train\n",
    "\n",
    "print('Size of the sequences:   ',Ls)\n",
    "print('Size of the data:        ',N)\n",
    "print('Size of the training set:',N_train)\n",
    "print('Size of the test set:    ',N_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just printing some datas to check it\n",
    "# I noticed this isn't the same rule in the lessons, so it is unkown\n",
    "for _ in range(10):\n",
    "    print(sx[_],sy[_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converting the label set to int values:\n",
    "print(sy[0],type(sy[0]))\n",
    "y = sy.astype(int)\n",
    "print(y[0],type(y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Letters and number of letters\n",
    "Q = ['A', 'C', 'G', 'T']\n",
    "Nc = 4\n",
    "\n",
    "# building the dictionary\n",
    "onehc = {Q[i]: i for i in range (Nc)}\n",
    "\n",
    "L = Ls * Nc # lenght of the input array (64)\n",
    "\n",
    "# The following cycle will transform the input vectors (for ex.: AAGGTCTGCCGGCCGA) in a\n",
    "# binary like way\n",
    "#\n",
    "# A = 1000\n",
    "# C = 0100\n",
    "# G = 0010\n",
    "# T = 0001\n",
    "#\n",
    "#   A    A    G    G    T    C   ...\n",
    "# 1000 1000 0010 0010 0001 0100\n",
    "x = np.zeros((N,L))\n",
    "for n in range(N): #for all the samples\n",
    "    for i in range(Ls): # for every character\n",
    "        x[n][i*4 + onehc[sx[n][i]]] = 1\n",
    "        \n",
    "print(sx[0])\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train) = (x[:N_train],y[:N_train])\n",
    "(x_test, y_test) = (x[N_train:],y[N_train:])\n",
    "\n",
    "# Check the fraction of datas equal to one\n",
    "print(y_train.sum() / N_train)\n",
    "print(y_test.sum() / N_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Is the model converging with a smaller database of samples with longer sequences? By converging we mean reducing significantly the validation loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I MADE THE SAME TRAINING IMPLEMENTING 3 FUNCTIONS\n",
    "# layer_i is an array representing a hidden layer used to create a neural network:\n",
    "# layer_i = [nodes, activation, dropout_ratio]\n",
    "# hidden_layers is a array of the hidden layers\n",
    "layer_1 = [L/2, 'relu', 0]\n",
    "layer_2 = [L/4, 'relu', .2]\n",
    "hidden_layers = [layer_1,layer_2]\n",
    "\n",
    "# optimization chosen\n",
    "opt = SGD(learning_rate=0.01, momentum=0.9, nesterov= True)\n",
    "\n",
    "# the first function creates the Neural Network, it needs the input dimension and the array of layers\n",
    "def createmodel(input_dim, hid_layers,activation = None, dropout = None):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # we add the first layer (input layer)\n",
    "    model.add(Dense(input_dim, input_shape=(input_dim,)))\n",
    "    \n",
    "    # we add the hidden layers\n",
    "    for layer in hid_layers:\n",
    "        act = activation or layer[1]\n",
    "        model.add(Dense(layer[0],activation=act))\n",
    "        drop = dropout or layer[2]\n",
    "        if drop: model.add(Dropout(drop))\n",
    "    \n",
    "    # we add the output layer\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "# the second function trains the model created with createmodel, as a input it needs the output of createmodels\n",
    "# the optimization and the arguments of model.fit() (except for shuffle)\n",
    "def trainmodel(model, opt, *args, **kwargs):\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    fit = model.fit(*args, **kwargs, shuffle = True)\n",
    "    '''fit = model.fit(x_train, y_train,\n",
    "                    epochs = 60, batch_size = 50,\n",
    "                    validation_data = (x_test,y_test),\n",
    "                    shuffle = True)'''\n",
    "    \n",
    "    return fit\n",
    "\n",
    "def createmodel_noprint(input_dim, hidden_layers):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # we add the first layer (input layer)\n",
    "    model.add(Dense(input_dim, input_shape=(input_dim,)))\n",
    "    \n",
    "    # we add the hidden layers\n",
    "    for layer in hidden_layers:\n",
    "        model.add(Dense(layer[0],activation=layer[1]))\n",
    "        if layer[2]:\n",
    "            model.add(Dropout(layer[2]))\n",
    "    \n",
    "    # we add the output layer\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "def trainmodel_noprint(model, opt, *args, **kwargs):\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    fit = model.fit(*args, **kwargs, shuffle = True, verbose = 0)\n",
    "    '''fit = model.fit(x_train, y_train,\n",
    "                    epochs = 60, batch_size = 50,\n",
    "                    validation_data = (x_test,y_test),\n",
    "                    shuffle = True)'''\n",
    "    \n",
    "    return fit\n",
    "\n",
    "# the third function takes the output of trainmodel and plot the loss of training data and test data\n",
    "def plotmodel(fit):\n",
    "    for obs in ('accuracy', 'loss'):\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(fit.history[obs], 'r', label = obs + ' of training data')\n",
    "        plt.plot(fit.history['val_'+obs], 'b--', label = obs + ' of test data')\n",
    "        plt.ylabel(obs)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define the model\n",
    "model = createmodel(L,hidden_layers)\n",
    "\n",
    "#Train the NN\n",
    "training = trainmodel(model, opt, x_train, y_train,\n",
    "          epochs=60, batch_size=50, validation_data= (x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "plotmodel(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Try to improve the performance of the DNN over the validation data set by “augmenting”\n",
    "the training data: For every sample there are L s -1 periodic shifts of the kind\n",
    "AAACCCTTTGGG→ GAAACCCTTTGG → GGAAACCCTTTG→ etc.\n",
    "We know that they can break the keys and provide a sample x’[n] with wrong label y[n]\n",
    "(which is the label of original sample x[n]), but they also enlarge the number of good\n",
    "samples for the DNN. Which of the two effects is prevalent?\n",
    "Is the situation improving by augmenting the training data from N t real samples to L s *N t ones with this procedure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function produces the permutations\n",
    "rot = lambda A: [A[i:]+A[:i] for i in range(len(A))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generate the augmented data\n",
    "sxx = []\n",
    "syy = np.zeros(len(sx)*Ls)\n",
    "for i in range(len(sx)):\n",
    "    sxx= np.append(sxx,rot(sx[i]))\n",
    "    syy[i*Ls:(i+1)*Ls] = sy[i]\n",
    "syy = syy.astype(int)\n",
    "\n",
    "#Now we permutate the samples \n",
    "permutation = np.random.permutation(sxx.shape[0])\n",
    "\n",
    "sxx = sxx[permutation] \n",
    "syy = syy[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(syy)\n",
    "\n",
    "Ls = len(sxx[0])\n",
    "\n",
    "perc_train = 0.8 # I kept the same train-test ratio as in the lesson\n",
    "N_train = int(N*perc_train)\n",
    "N_test = N - N_train\n",
    "\n",
    "print('Size of the sequences:   ',Ls)\n",
    "print('Size of the data:        ',N)\n",
    "print('Size of the training set:',N_train)\n",
    "print('Size of the test set:    ',N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = syy\n",
    "x = np.zeros((N,L))\n",
    "for n in range(N): #for all the samples\n",
    "    for i in range(Ls): # for every character\n",
    "        x[n][i*4 + onehc[sxx[n][i]]] = 1\n",
    "        \n",
    "print(sxx[0])\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train) = (x[:N_train],y[:N_train])\n",
    "(x_test, y_test) = (x[N_train:],y[N_train:])\n",
    "\n",
    "# Check the fraction of datas equal to one\n",
    "print(y_train.sum() / N_train)\n",
    "print(y_test.sum() / N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define the model\n",
    "model = createmodel(L,hidden_layers)\n",
    "\n",
    "#Train the NN\n",
    "training = trainmodel(model, opt, x_train, y_train,\n",
    "          epochs=60, batch_size=50, validation_data= (x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "plotmodel(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "plotmodel(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implement a “grid search” as shown in NB11 to improve one or more of the aspects or\n",
    "parameters of the model. Possible tests include: different activation units (sigmoid, relu, elu,\n",
    "etc.), different minimization algorithms (ADAM, RMSprop, Nesterov, etc.), different\n",
    "dropouts, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compile_model(optimizer, activation = None, dropout = None, learning_rate = 0.1):\n",
    "    model = createmodel(L,hidden_layers, activation = activation, dropout = dropout)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer(learning_rate = learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Using Keras scikit wrapper\n",
    "model_gridsearch = KerasClassifier(build_fn=compile_model, \n",
    "                        epochs=1,\n",
    "                        batch_size = 50, \n",
    "                        verbose=1)\n",
    "\n",
    "optimizer = [SGD, RMSprop]#, 'Adam', 'Adamax']\n",
    "dropout = [0.1,]#0.3,0.5]\n",
    "learning_rate = [0.01,]#0.05,0.1]\n",
    "activation = ['relu',]# 'elu']\n",
    "# parameters dictionary\n",
    "param_grid = dict(optimizer=optimizer, activation = activation, dropout = dropout, learning_rate = learning_rate)\n",
    "# Run gridsearch\n",
    "grid = GridSearchCV(estimator=model_gridsearch, param_grid=param_grid, n_jobs=-1, cv=4)\n",
    "grid_result = grid.fit(x_train,y_train)\n",
    "\n",
    "# summarize results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "best_model = grid_result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize gridsearch results\n",
    "print(\"Best score: %f \" % (grid_result.best_score_))\n",
    "print(\"Obtained using %s\" % (grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "print('')\n",
    "print('')\n",
    "print('General results for the gridsearch')\n",
    "print('')\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"Test_score: %f (+/- %f) using: \" % (mean, stdev))\n",
    "    print(param)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. See if any rescaling of data may improve the results. For instance one may use [-0.5,+0.5]\n",
    "instead of [0,1] for every bit of x[n]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally as a preprocessing operation the data is rescaled, here we see the importance of this preprocessing\n",
    "# operation.\n",
    "# First we center our data around zero:\n",
    "#          [0,1] --> [-1.5,1.5]\n",
    "\n",
    "rescfactor_x = 3\n",
    "shift_x = 0\n",
    "\n",
    "# Rescaling transformation\n",
    "resc_x = (x - .5 )* rescfactor_x + shift_x # easy enough\n",
    "\n",
    "# y remains the same\n",
    "resc_y = (y)\n",
    "\n",
    "# print the inputs just as a check\n",
    "print(resc_x)\n",
    "\n",
    "(resc_x_train, resc_y_train) = (resc_x[:N_train],resc_y[:N_train])\n",
    "(resc_x_test, resc_y_test) = (resc_x[N_train:],resc_y[N_train:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TO BE MODIFIED \n",
    "best_hidden_layers = hidden_layers\n",
    "best_opt = opt\n",
    "\n",
    "resc_model = createmodel(L,best_hidden_layers)\n",
    "\n",
    "# Training of the model with the rescaled data\n",
    "resc_training = trainmodel_noprint(resc_model, best_opt, resc_x_train, resc_y_train,\n",
    "          epochs=60, batch_size=50, validation_data= (resc_x_test, resc_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "resc_score = resc_model.evaluate(resc_x_test, resc_y_test, verbose=1)\n",
    "\n",
    "plotmodel(resc_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test loss:                  ', round(score[0],5))\n",
    "print('Rescaled Data Test loss:    ', round(resc_score[0],5))\n",
    "\n",
    "print('\\nTest accuracy:              ', round(score[1],5))\n",
    "print('Rescaled Data Test accuracy:', round(resc_score[1],5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we rescale the data to be far away from zero, we should expect less quality of the training due to \n",
    "# Gradient Vanishing\n",
    "rescfactor_x_2 = 3\n",
    "shift_x_2 = 10\n",
    "\n",
    "# Rescaling transformation\n",
    "resc_x_2 = (x - .5 )* rescfactor_x_2 + shift_x_2 # easy enough\n",
    "\n",
    "# y remains the same\n",
    "resc_y_2 = (y)\n",
    "\n",
    "# print the inputs just as a check\n",
    "print(resc_x_2)\n",
    "\n",
    "(resc_x_2_train, resc_y_2_train) = (resc_x_2[:N_train],resc_y_2[:N_train])\n",
    "(resc_x_2_test, resc_y_2_test) = (resc_x_2[N_train:],resc_y_2[N_train:])\n",
    "\n",
    "resc_model = createmodel(L,best_hidden_layers)\n",
    "\n",
    "# Training of the model with the rescaled data\n",
    "resc_2_training = trainmodel_noprint(resc_model, best_opt, resc_x_2_train, resc_y_2_train,\n",
    "          epochs=60, batch_size=50, validation_data= (resc_x_2_test, resc_y_2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resc_2_score = resc_model.evaluate(resc_x_2_test, resc_y_2_test, verbose=1)\n",
    "\n",
    "plotmodel(resc_2_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test loss:                  ', round(score[0],5))\n",
    "print('Rescaled Data Test loss:    ', round(resc_2_score[0],5))\n",
    "\n",
    "print('\\nTest accuracy:              ', round(score[1],5))\n",
    "print('Rescaled Data Test accuracy:', round(resc_2_score[1],5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
