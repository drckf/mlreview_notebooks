{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCPB 20-21 exercise 3\n",
    "\n",
    "### Saverio Monaco\n",
    "\n",
    "### Gerardo Carmona\n",
    "\n",
    "### Hilario Capettini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras import optimizers\n",
    "from keras import initializers,regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_import(str0):\n",
    "    fnamex =\"DATA/x_\" + str0 + \".csv\"\n",
    "    fnamey =\"DATA/y_\" + str0 + \".csv\"\n",
    "\n",
    "    x = np.loadtxt(fnamex,delimiter=',',dtype=float)\n",
    "    N = len(x)\n",
    "    print('Length of x = ',N)\n",
    "\n",
    "\n",
    "\n",
    "    categ = np.loadtxt(fnamey,delimiter=',', dtype=int)\n",
    "    #Number of categories\n",
    "    N_categ = len(np.unique(categ))\n",
    "    y = np.zeros((N,N_categ))\n",
    "\n",
    "    for n in range(N):\n",
    "        y[n][categ[n]] = 1\n",
    "     \n",
    "    \n",
    "    return x,y,N\n",
    "    \n",
    "\n",
    "def set_spliting(N,x,y,perc_train = 0.8):  \n",
    "    N_train = int(N * perc_train)\n",
    "    N_val = N-N_train\n",
    "\n",
    "    x_train = x[:N_train]\n",
    "    y_train = y[:N_train]\n",
    "\n",
    "    x_val = x[N_train:]\n",
    "    y_val = y[N_train:]\n",
    "    \n",
    "    L =len(x[0])\n",
    "    \n",
    "    \n",
    "    x_train = x_train.reshape(x_train.shape[0],L,1)\n",
    "    x_val = x_val.reshape(x_val.shape[0],L,1)\n",
    "    input_shape = (L,1)\n",
    "\n",
    "    return x_train, y_train, x_val, y_val, input_shape\n",
    "    \n",
    "def data_process(x,N):\n",
    "    xm = x.mean(axis=1)\n",
    "    for n in range(N):\n",
    "        x[n] = x[n]-xm[n]\n",
    "    \n",
    "    std = x.std(axis=1)\n",
    "    for n in range(N):\n",
    "        x[n] = x[n] / std[n]\n",
    "    return x\n",
    "\n",
    "def build_model(first_layer,deep_layers,output_size,output_activation = 'softmax', opt = optimizers.Adam()):\n",
    "    model = Sequential()\n",
    "    model.add(first_layer)\n",
    "    for layer in layers: model.add(layer)\n",
    "    model.add(Dense(output_size,activation = output_activation))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusionmatrix(y_pred, y_true, normalization = False):\n",
    "    # Insted of having y = [0,0,1] , [0,1,0] and [1,0,0] as labels, we transform them in\n",
    "    # y = 0, 1 ,2\n",
    "    y_true_mod = np.argmax(y_true, axis=1) #tells wich of the 3 argument is max\n",
    "    # same for the prediction\n",
    "    y_pred_mod = np.argmax(y_pred, axis=1)\n",
    "    # N.B: if we don't do that we get the following error: \n",
    "    # ValueError: Classification metrics can't handle a mix of continuous-multioutput and multilabel-indicator targets\n",
    "    conf_mat = confusion_matrix(y_pred_mod, y_true_mod) # doing the same makes sense, we take the argument\n",
    "                                                        # with the highest value (i.e more probable)\n",
    "    if normalization:\n",
    "        norm = np.sum(np.sum(conf_mat,axis=1))\n",
    "        conf_mat = conf_mat / norm\n",
    "        #print(norm)\n",
    "        # check that it is normalized\n",
    "        #print(np.sum(np.sum(conf_mat,axis=0)))\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    if not normalization:\n",
    "        sns.heatmap(conf_mat, annot=True, fmt='d', cmap = \"mako\")\n",
    "    if normalization:\n",
    "        sns.heatmap(conf_mat, annot=True, fmt='f', cmap = \"mako\")\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracyloss(fit, title = False):\n",
    "    fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(13, 5))\n",
    "    axs = [ax1,ax2]\n",
    "    obs = ['accuracy','loss']\n",
    "    for i in range(2):\n",
    "        axs[i].plot(fit.history[obs[i]],'r',label =obs[i]+' of training data')\n",
    "        #dashed line!!\n",
    "        axs[i].plot(fit.history['val_'+obs[i]],'b--',\n",
    "            label =obs[i]+' of validation data')\n",
    "        axs[i].set_ylabel(obs[i])\n",
    "        axs[i].set_xlabel('epochs')\n",
    "        if title:\n",
    "            axs[i].set_title(title)\n",
    "        axs[i].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File importation\n",
    "str0=\"ts_L60_Z12_A500_DX50_bias5_N10000\"\n",
    "x, y, N = data_import(str0)\n",
    "x = data_process(x,N)\n",
    "x_train, y_train, x_val, y_val, input_shape = set_spliting(N,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data visualization\n",
    "plt.title('Examples of Input signals')\n",
    "plt.plot(x[0])\n",
    "plt.plot(x[1])\n",
    "plt.plot(x[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = regularizers.l1(0.01)    \n",
    "#Now we initialize the NN weights\n",
    "ini = initializers.RandomNormal(mean=0,stddev=0.05)\n",
    "conv_lay = lambda shape: Conv1D(filters =5, kernel_size =11, kernel_regularizer = reg, kernel_initializer = ini, activation = 'relu', input_shape = shape)\n",
    "\n",
    "layers = [\n",
    "    AveragePooling1D(5),\n",
    "    Conv1D(filters=5,kernel_size=7,activation=\"relu\"),\n",
    "    Flatten(),\n",
    "    Dense(10,activation='relu'),\n",
    "    Dropout(0.2)]\n",
    "\n",
    "\n",
    "model = build_model(conv_lay(input_shape), layers, 3)\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size =250\n",
    "epochs = 10\n",
    "\n",
    "fit = model.fit(x_train,y_train,\n",
    "               batch_size=batch_size,\n",
    "               epochs = epochs,\n",
    "               validation_data =(x_val,y_val),\n",
    "               verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(x_train)\n",
    "plot_confusionmatrix(results,y_train, normalization = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_accuracyloss(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 \n",
    "By reducing the signal-to-noise ratio, namely the\n",
    "amplitude of the external signal in the data (A) with\n",
    "respect to the step typical size DX of the jump process,\n",
    "check where the network (defined in the class) starts to\n",
    "fail discriminating the categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,y1,N1 = data_import(\"ts_L60_Z12_A10_DX50_bias5_N10000\")\n",
    "x2,y2,N2 = data_import(\"ts_L60_Z12_A500_DX50_bias5_N10000\")\n",
    "x3,y3,N3 = data_import(\"ts_L60_Z12_A1000_DX50_bias5_N10000\")\n",
    "x4,y4,N4 = data_import(\"ts_L60_Z12_A10000_DX50_bias5_N10000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2,ax3,ax4) = plt.subplots(nrows=4, ncols=1, figsize=(12, 12))\n",
    "\n",
    "xs  = [x1 ,x2 ,x3 ,x4]\n",
    "axs = [ax1,ax2,ax3,ax4]\n",
    "colors = ['red','blue','black','forestgreen']\n",
    "\n",
    "for k in range(len(xs)): \n",
    "    for j in range(20):\n",
    "        axs[k].plot([i for i in range(j*len(xs[k][0]),(len(xs[k][0])+j*len(xs[k][0])))],xs[k][j],color=colors[k])\n",
    "        axs[k].set_ylabel('Amplitude', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = data_process(x1,N1)\n",
    "x2 = data_process(x2,N2)\n",
    "x3 = data_process(x3,N3)\n",
    "x4 = data_process(x4,N4)\n",
    "\n",
    "x1_train, y1_train, x1_val, y1_val, input_shape1 = set_spliting(N1,x1,y1,perc_train = 0.8)\n",
    "x2_train, y2_train, x2_val, y2_val, input_shape2 = set_spliting(N2,x2,y2,perc_train = 0.8)\n",
    "x3_train, y3_train, x3_val, y3_val, input_shape3 = set_spliting(N3,x3,y3,perc_train = 0.8)\n",
    "x4_train, y4_train, x4_val, y4_val, input_shape4 = set_spliting(N4,x4,y4,perc_train = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size =250\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "fit1 = build_model(conv_lay(input_shape1),layers,3).fit(x1_train,y1_train,\n",
    "               batch_size=batch_size,\n",
    "               epochs = epochs,\n",
    "               validation_data =(x1_val,y1_val),\n",
    "               verbose=2, shuffle=True)\n",
    "\n",
    "fit2 = build_model(conv_lay(input_shape2),layers,3).fit(x2_train,y2_train,\n",
    "               batch_size=batch_size,\n",
    "               epochs = epochs,\n",
    "               validation_data =(x2_val,y2_val),\n",
    "               verbose=2, shuffle=True)\n",
    "\n",
    "fit3 = build_model(conv_lay(input_shape3),layers,3).fit(x3_train,y3_train,\n",
    "               batch_size=batch_size,\n",
    "               epochs = epochs,\n",
    "               validation_data =(x3_val,y3_val),\n",
    "               verbose=2, shuffle=True)\n",
    "\n",
    "fit4 = build_model(conv_lay(input_shape4),layers,3).fit(x4_train,y4_train,\n",
    "               batch_size=batch_size,\n",
    "               epochs = epochs,\n",
    "               validation_data =(x4_val,y4_val),\n",
    "               verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits =        [fit1,fit2,fit3,fit4]\n",
    "pattern_mag = [10,100,1000,10000]\n",
    "for i in range(len(fits)):\n",
    "    plot_accuracyloss(fits[i],title='A='+str(pattern_mag[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Losses with various noise-signal ratios on validation data')\n",
    "for i in range(len(fits)):\n",
    "    plt.plot(fits[i].history['val_loss'],c=colors[i],label='A='+str(pattern_mag[i]))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Try another version where only one convolutional layer is\n",
    "introduced instead of two, and where any number of\n",
    "dense layers may be used, with the global constraint of\n",
    "using a network with at most $600$ trainable parameters.\n",
    "\n",
    "Is the number of parameters scaling more quickly by adding Dense layers or Conv1D\n",
    "layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "str0=\"ts_comp_N10000\"\n",
    "\n",
    "x, y, N = data_import(str0)\n",
    "x = data_process(x,N)\n",
    "x_train, y_train, x_val, y_val, input_shape = set_spliting(N,x,y)\n",
    "output_size =len(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "reg = regularizers.l1(0.01)\n",
    "#Now we initialize the NN weights\n",
    "ini = initializers.RandomNormal(mean=0,stddev=0.05)\n",
    "layers = [AveragePooling1D(5),\n",
    "          Flatten(),\n",
    "          Dense(6,activation='relu'),\n",
    "          Dropout(0.2),\n",
    "          Dense(12,activation='relu'),\n",
    "          Dense(9,activation='relu')]\n",
    "\n",
    "model_1 = build_model(conv_lay(input_shape), layers, output_size)\n",
    "\n",
    "print(model_1.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size =250\n",
    "epochs = 200\n",
    "\n",
    "fit_1 = model_1.fit(x_train,y_train,\n",
    "               batch_size=batch_size,\n",
    "               epochs = epochs,\n",
    "               validation_data =(x_val,y_val),\n",
    "               verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_1.predict(x_train)\n",
    "plot_confusionmatrix(results,y_train, normalization = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracyloss(fit_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Check if regularization may improve the performances by varying the parameter lambda of\n",
    "the L1 (LASSO) or of the L2 (Ridge) regularization; see lambda in eqs.(43) and (52) in the\n",
    "review. There is also a mixed version (l1_l2) that can be tried.\n",
    "\n",
    "    1. Is there any intermediate value of lambda where the performances of the network are better?\n",
    "\n",
    "    2. Is there any improvement in the visualization and understanding of the weights in the filters?\n",
    "\n",
    "Note that the regularization we introduced acts on the wâ€™s, not on the biases. One can also\n",
    "try the equivalent procedure for biases or for the output of the relu units (see Keras doc.), if\n",
    "there is any reason for suspecting that it may help. In our case, the logic was to let the\n",
    "weights of the filters go to zero if not needed, hence that kind of regularization was selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1_l2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "lam = np.logspace(0,-4,5).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_conv_lay(l1,l2):\n",
    "    return Conv1D(filters =5, kernel_size =11, kernel_regularizer = l1_l2(l1 = l1, l2 = l2), kernel_initializer = ini, activation = 'relu', input_shape = input_shape)\n",
    "\n",
    "def compile_model(l1,l2):\n",
    "\n",
    "    model = build_model(reg_conv_lay(l1,l2),layers,output_size)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Using Keras scikit wrapper\n",
    "model_gridsearch = KerasClassifier(build_fn=compile_model, \n",
    "                        epochs=200,\n",
    "                        batch_size = 250, \n",
    "                        verbose=2)\n",
    "\n",
    "# parameters dictionary\n",
    "param_grid = dict(l1 = lam,l2=lam)\n",
    "# Run gridsearch\n",
    "grid = GridSearchCV(estimator=model_gridsearch, param_grid=param_grid, n_jobs=-1, cv=4)\n",
    "grid_result = grid.fit(x_train,y_train)\n",
    "\n",
    "# summarize results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "best_model = grid_result.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRA: If there is time, draw the confusion matrix for a CNN model trained with the sequence data of the\n",
    "exercise 02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datas\n",
    "ex_fname_x = \"DATA/sequences16_x_augmented.csv\"\n",
    "ex_fname_y = \"DATA/sequences16_y_augmented.csv\"\n",
    "\n",
    "ex_x = np.loadtxt(ex_fname_x,delimiter=',', dtype=int)\n",
    "ex_y = np.loadtxt(ex_fname_y,delimiter=',', dtype=int)\n",
    "\n",
    "#print(ex_x)\n",
    "#print(ex_y)\n",
    "ex_N = np.shape(ex_y)[0]\n",
    "#print(ex_N)\n",
    "\n",
    "ex_x_train, ex_y_train, ex_x_val, ex_y_val, ex_input_shape = set_spliting(ex_N,ex_x,ex_y,perc_train = 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we train the best CNN using the data of ex2\n",
    "\n",
    "## TODO: build a model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_results = ex_model.predict(ex_x_train)\n",
    "plot_confusionmatrix(ex_results,ex_y_train, normalization = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
