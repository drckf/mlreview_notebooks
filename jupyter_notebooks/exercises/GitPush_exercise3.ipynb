{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File importation\n",
    "str0=\"ts_L60_Z12_A500_DX50_bias5_N10000\"\n",
    "fnamex =\"DATA/x_\" + str0 + \".csv\"\n",
    "fnamey =\"DATA/y_\" + str0 + \".csv\"\n",
    "\n",
    "x = np.loadtxt(fnamex,delimiter=',',dtype=float)\n",
    "N = len(x)\n",
    "print('Length of x = ',N)\n",
    "\n",
    "\n",
    "categ = np.loadtxt(fnamey,delimiter=',', dtype=int)\n",
    "#Number of categories\n",
    "N_categ = 3\n",
    "y = np.zeros((N,N_categ))\n",
    "\n",
    "for n in range(N):\n",
    "    y[n][categ[n]] = 1\n",
    "    \n",
    "print(y[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We saw that the nn was not converging and it might be\n",
    "#due to some average, so we are going to remove it\n",
    "\n",
    "xm = x.mean(axis=1)\n",
    "for n in range(N):\n",
    "    x[n] = x[n]-xm[n]\n",
    "    \n",
    "std = x.std(axis=1)\n",
    "for n in range(N):\n",
    "    x[n] = x[n] / std[n]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data visualization\n",
    "plt.plot(x[0])\n",
    "plt.plot(x[1])\n",
    "plt.plot(x[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We asign the data for training and test\n",
    "perc_train = 0.8\n",
    "N_train = int(N * perc_train)\n",
    "N_val = N-N_train\n",
    "\n",
    "x_train = x[:N_train]\n",
    "y_train = y[:N_train]\n",
    "\n",
    "x_val = x[N_train:]\n",
    "y_val = y[N_train:]\n",
    "\n",
    "L =len(x[0])\n",
    "\n",
    "print('Number of samples',N)\n",
    "print('Training samples',N_train)\n",
    "print('Validation samples',N_val)\n",
    "print('Length of x[0]',L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0],L,1)\n",
    "x_val = x_val.reshape(x_val.shape[0],L,1)\n",
    "input_shape = (L,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import initializers,regularizers\n",
    "np.random.seed(12345)\n",
    "\n",
    "#We use l1 for Lazzo regularization\n",
    "reg = regularizers.l1(0.1)   \n",
    "\n",
    "#Now we initialize the NN weights\n",
    "ini = initializers.RandomNormal(mean=0,stddev=0.05)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters =5, kernel_size =11, \n",
    "                 kernel_regularizer = reg,\n",
    "                 kernel_initializer = ini,\n",
    "                 activation = 'relu',\n",
    "                 input_shape =input_shape))\n",
    "\n",
    "model.add(AveragePooling1D(5))\n",
    "model.add(Conv1D(filters=5,kernel_size=7,activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(N_categ,activation='softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "from keras import optimizers\n",
    "opt =optimizers.SGD(lr=0.01,momentum=0.9,nesterov=True,decay=1.e6)\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "             optimizer='Adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size =250\n",
    "epochs = 30\n",
    "\n",
    "fit = model.fit(x_train,y_train,\n",
    "               batch_size=batch_size,\n",
    "               epochs = epochs,\n",
    "               validation_data =(x_val,y_val),\n",
    "               verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obs in ('accuracy','loss'):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(fit.history[obs],'r',\n",
    "            label =obs+' of training data')\n",
    "    \n",
    "    #dashed line!!\n",
    "    plt.plot(fit.history['val_'+obs],'b--',\n",
    "            label =obs+' of validation data')\n",
    "    \n",
    "    plt.ylabel(obs)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "3. Try another version where only one convolutional layer is\n",
    "introduced instead of two, and where any number of\n",
    "dense layers may be used, with the global constraint of\n",
    "using a network with at most $600$ trainable parameters.\n",
    "\n",
    "Is the number of parameters scaling more quickly by adding Dense layers or Conv1D\n",
    "layers?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "name_key = lambda a, name: f'DATA/{a}_{name}.csv'\n",
    "#File importation\n",
    "def get_data(file_suffix):\n",
    "    xfile = name_key('x',file_suffix)\n",
    "    yfile = name_key('y',file_suffix)\n",
    "    x = np.loadtxt(xfile,delimiter=',',dtype=float)\n",
    "    y = np.loadtxt(yfile,delimiter=',',dtype=int)\n",
    "    return x, y\n",
    "\n",
    "def reshape_data(x,y, normalize = False):\n",
    "    # Convert y\n",
    "    N = len(x)\n",
    "    N_cl = len(np.unique(y))\n",
    "    _y = np.zeros((N,N_cl))\n",
    "    for i in range(N): _y[i][y[i]] = 1\n",
    "    \n",
    "    perm = np.random.permutation(N)\n",
    "    _x = x[perm]\n",
    "    _y = _y[perm]\n",
    "\n",
    "    if normalize:\n",
    "        xm, std = _x.mean(axis=1), _x.std(axis=1)#should it be -1?\n",
    "        for i in range(N): _x[i] = (_x[i]-xm[i])/std[i]\n",
    "    \n",
    "    return _x, _y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "str0=\"ts_comp_N10000\"\n",
    "\n",
    "x, y = get_data(str0)\n",
    "x, y = reshape_data(x,y,True)\n",
    "\n",
    "N = len(x)\n",
    "\n",
    "# Balance the datasets\n",
    "perc_train = 0.8\n",
    "N_train = int(N * perc_train)\n",
    "N_val = N-N_train\n",
    "\n",
    "x_train = x[:N_train]\n",
    "y_train = y[:N_train]\n",
    "\n",
    "x_val = x[N_train:]\n",
    "y_val = y[N_train:]\n",
    "\n",
    "L =len(x[0])\n",
    "\n",
    "print('Number of samples',N)\n",
    "print('Training samples',N_train)\n",
    "print('Validation samples',N_val)\n",
    "print('Length of x[0]',L)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0],L,1)\n",
    "x_val = x_val.reshape(x_val.shape[0],L,1)\n",
    "input_shape = (L,1)\n",
    "output_size = len(y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "def build_model(layers,output_size,output_activation = 'softmax'):\n",
    "    model = Sequential()\n",
    "    for layer in layers: model.add(layer)\n",
    "    model.add(Dense(output_size,activation = output_activation))\n",
    "    return model\n",
    "\n",
    "layers = [Conv1D(filters =5, kernel_size =11, \n",
    "                 kernel_regularizer = reg,\n",
    "                 kernel_initializer = ini,\n",
    "                 activation = 'relu',\n",
    "                 input_shape =input_shape),\n",
    "          AveragePooling1D(5),\n",
    "          Flatten(),\n",
    "          Dense(6,activation='relu'),\n",
    "          Dropout(0.2),\n",
    "          Dense(12,activation='relu'),\n",
    "          Dense(8,activation='relu')]\n",
    "\n",
    "model_1 = build_model(layers, output_size)\n",
    "\n",
    "from keras import optimizers\n",
    "opt =optimizers.Adam()\n",
    "\n",
    "model_1.compile(loss=keras.losses.categorical_crossentropy,\n",
    "             optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "print(model_1.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size =250\n",
    "epochs = 300\n",
    "\n",
    "fit = model_1.fit(x_train,y_train,\n",
    "               batch_size=batch_size,\n",
    "               epochs = epochs,\n",
    "               validation_data =(x_val,y_val),\n",
    "               verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python38564bit075de96200f448aeb6d0298b6117be4d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}