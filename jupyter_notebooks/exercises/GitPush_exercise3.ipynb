{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras import optimizers\n",
    "from keras import initializers,regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File importation\n",
    "str0=\"ts_L60_Z12_A500_DX50_bias5_N10000\"\n",
    "fnamex =\"DATA/x_\" + str0 + \".csv\"\n",
    "fnamey =\"DATA/y_\" + str0 + \".csv\"\n",
    "\n",
    "x = np.loadtxt(fnamex,delimiter=',',dtype=float)\n",
    "N = len(x)\n",
    "print('Length of x = ',N)\n",
    "\n",
    "\n",
    "\n",
    "categ = np.loadtxt(fnamey,delimiter=',', dtype=int)\n",
    "#Number of categories\n",
    "N_categ = 3\n",
    "y = np.zeros((N,N_categ))\n",
    "\n",
    "for n in range(N):\n",
    "    y[n][categ[n]] = 1\n",
    "    \n",
    "print(y[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We saw that the nn was not converging and it might be\n",
    "#due to some average, so we are going to remove it\n",
    "\n",
    "xm = x.mean(axis=1)\n",
    "for n in range(N):\n",
    "    x[n] = x[n]-xm[n]\n",
    "    \n",
    "std = x.std(axis=1)\n",
    "for n in range(N):\n",
    "    x[n] = x[n] / std[n]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data visualization\n",
    "plt.plot(x[0])\n",
    "plt.plot(x[1])\n",
    "plt.plot(x[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We asign the data for training and test\n",
    "perc_train = 0.8\n",
    "N_train = int(N * perc_train)\n",
    "N_val = N-N_train\n",
    "\n",
    "x_train = x[:N_train]\n",
    "y_train = y[:N_train]\n",
    "\n",
    "x_val = x[N_train:]\n",
    "y_val = y[N_train:]\n",
    "\n",
    "L =len(x[0])\n",
    "\n",
    "print('Number of samples',N)\n",
    "print('Training samples',N_train)\n",
    "print('Validation samples',N_val)\n",
    "print('Length of x[0]',L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0],L,1)\n",
    "x_val = x_val.reshape(x_val.shape[0],L,1)\n",
    "input_shape = (L,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import initializers,regularizers\n",
    "np.random.seed(12345)\n",
    "\n",
    "#We use l1 for Lazzo regularization\n",
    "reg = regularizers.l1(0.01)    \n",
    "\n",
    "#Now we initialize the NN weights\n",
    "ini = initializers.RandomNormal(mean=0,stddev=0.05)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters =5, kernel_size =11, \n",
    "                 kernel_regularizer = reg,\n",
    "                 kernel_initializer = ini,\n",
    "                 activation = 'relu',\n",
    "                 input_shape =input_shape))\n",
    "\n",
    "model.add(AveragePooling1D(5))\n",
    "model.add(Conv1D(filters=5,kernel_size=7,activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(N_categ,activation='softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "opt =optimizers.Adam()\n",
    "#opt = optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov= True)\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "             optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size =250\n",
    "epochs = 100\n",
    "\n",
    "fit = model.fit(x_train,y_train,\n",
    "               batch_size=batch_size,\n",
    "               epochs = epochs,\n",
    "               validation_data =(x_val,y_val),\n",
    "               verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obs in ('accuracy','loss'):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(fit.history[obs],'r',\n",
    "            label =obs+' of training data')\n",
    "    \n",
    "    #dashed line!!\n",
    "    plt.plot(fit.history['val_'+obs],'b--',\n",
    "            label =obs+' of validation data')\n",
    "    \n",
    "    plt.ylabel(obs)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 \n",
    "By reducing the signal-to-noise ratio, namely the\n",
    "amplitude of the external signal in the data (A) with\n",
    "respect to the step typical size DX of the jump process,\n",
    "check where the network (defined in the class) starts to\n",
    "fail discriminating the categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_import(str0):\n",
    "    fnamex =\"DATA/x_\" + str0 + \".csv\"\n",
    "    fnamey =\"DATA/y_\" + str0 + \".csv\"\n",
    "\n",
    "    x = np.loadtxt(fnamex,delimiter=',',dtype=float)\n",
    "    N = len(x)\n",
    "    print('Length of x = ',N)\n",
    "\n",
    "\n",
    "\n",
    "    categ = np.loadtxt(fnamey,delimiter=',', dtype=int)\n",
    "    #Number of categories\n",
    "    N_categ = 3\n",
    "    y = np.zeros((N,N_categ))\n",
    "\n",
    "    for n in range(N):\n",
    "        y[n][categ[n]] = 1\n",
    "     \n",
    "    \n",
    "    return x,y,N\n",
    "    \n",
    "\n",
    "def set_spliting(N,x,y,perc_train = 0.8):  \n",
    "    N_train = int(N * perc_train)\n",
    "    N_val = N-N_train\n",
    "\n",
    "    x_train = x[:N_train]\n",
    "    y_train = y[:N_train]\n",
    "\n",
    "    x_val = x[N_train:]\n",
    "    y_val = y[N_train:]\n",
    "    \n",
    "    L =len(x[0])\n",
    "    \n",
    "    \n",
    "    x_train = x_train.reshape(x_train.shape[0],L,1)\n",
    "    x_val = x_val.reshape(x_val.shape[0],L,1)\n",
    "    input_shape = (L,1)\n",
    "\n",
    "    return x_train, y_train, x_val, y_val, input_shape\n",
    "    \n",
    "def data_process(x,N):\n",
    "    xm = x.mean(axis=1)\n",
    "    for n in range(N):\n",
    "        x[n] = x[n]-xm[n]\n",
    "    \n",
    "    std = x.std(axis=1)\n",
    "    for n in range(N):\n",
    "        x[n] = x[n] / std[n]\n",
    "    return x\n",
    "\n",
    "def compilation(input_shape,N_categ=3):\n",
    "    np.random.seed(12345)\n",
    "\n",
    "    #We use l1 for Lazzo regularization\n",
    "    reg = regularizers.l1(0.01)    \n",
    "\n",
    "        #Now we initialize the NN weights\n",
    "    ini = initializers.RandomNormal(mean=0,stddev=0.05)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters =5, kernel_size =11, \n",
    "                     kernel_regularizer = reg,\n",
    "                     kernel_initializer = ini,\n",
    "                     activation = 'relu',\n",
    "                     input_shape =input_shape))\n",
    "\n",
    "    model.add(AveragePooling1D(5))\n",
    "    model.add(Conv1D(filters=5,kernel_size=7,activation=\"relu\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10,activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(N_categ,activation='softmax'))\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "   \n",
    "\n",
    "    opt =optimizers.Adam()\n",
    "    #opt = optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov= True)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                 optimizer=opt,metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,y1,N1 = data_import(\"ts_L60_Z12_A10_DX50_bias5_N10000\")\n",
    "x2,y2,N2 = data_import(\"ts_L60_Z12_A500_DX50_bias5_N10000\")\n",
    "x3,y3,N3 = data_import(\"ts_L60_Z12_A1000_DX50_bias5_N10000\")\n",
    "x4,y4,N4 = data_import(\"ts_L60_Z12_A10000_DX50_bias5_N10000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2,ax3,ax4) = plt.subplots(nrows=4, ncols=1, figsize=(12, 12))\n",
    "\n",
    "for j in range(20):\n",
    "    ax1.plot([i for i in range(j*len(x1[0]),(len(x1[0])+j*len(x1[0])))],x1[j],color='red')\n",
    "ax1.set_ylabel('Amplitude', fontsize=10)\n",
    "\n",
    "for j in range(20):\n",
    "    ax2.plot([i for i in range(j*len(x2[0]),(len(x2[0])+j*len(x2[0])))],x2[j],color='blue')\n",
    "ax2.set_ylabel('Amplitude', fontsize=10)\n",
    "\n",
    "for j in range(20):\n",
    "    ax3.plot([i for i in range(j*len(x3[0]),(len(x3[0])+j*len(x3[0])))],x3[j],color='black')\n",
    "ax3.set_ylabel('Amplitude', fontsize=10)\n",
    "\n",
    "for j in range(20):\n",
    "    ax4.plot([i for i in range(j*len(x4[0]),(len(x4[0])+j*len(x4[0])))],x4[j],color='orange')\n",
    "ax4.set_xlabel('Time', fontsize=20)\n",
    "ax4.set_ylabel('Amplitude', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = data_process(x1,N1)\n",
    "x2 = data_process(x2,N2)\n",
    "x3 = data_process(x3,N3)\n",
    "x4 = data_process(x4,N4)\n",
    "\n",
    "x1_train, y1_train, x1_val, y1_val, input_shape1 = set_spliting(N1,x1,y1,perc_train = 0.8)\n",
    "x2_train, y2_train, x2_val, y2_val, input_shape2 = set_spliting(N2,x2,y2,perc_train = 0.8)\n",
    "x3_train, y3_train, x3_val, y3_val, input_shape3 = set_spliting(N3,x3,y3,perc_train = 0.8)\n",
    "x4_train, y4_train, x4_val, y4_val, input_shape4 = set_spliting(N4,x4,y4,perc_train = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size =250\n",
    "epochs = 100\n",
    "\n",
    "fit1 = compilation(input_shape1,N_categ=3).fit(x1_train,y1_train,\n",
    "               batch_size=batch_size,\n",
    "               epochs = epochs,\n",
    "               validation_data =(x1_val,y1_val),\n",
    "               verbose=2, shuffle=True)\n",
    "\n",
    "fit2 = compilation(input_shape2,N_categ=3).fit(x2_train,y2_train,\n",
    "               batch_size=batch_size,\n",
    "               epochs = epochs,\n",
    "               validation_data =(x2_val,y2_val),\n",
    "               verbose=2, shuffle=True)\n",
    "\n",
    "fit3 = compilation(input_shape3,N_categ=3).fit(x3_train,y3_train,\n",
    "               batch_size=batch_size,\n",
    "               epochs = epochs,\n",
    "               validation_data =(x3_val,y3_val),\n",
    "               verbose=2, shuffle=True)\n",
    "\n",
    "fit4 = compilation(input_shape4,N_categ=3).fit(x4_train,y4_train,\n",
    "               batch_size=batch_size,\n",
    "               epochs = epochs,\n",
    "               validation_data =(x4_val,y4_val),\n",
    "               verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1,ax2),(ax3,ax4),(ax5,ax6),(ax7,ax8)) = plt.subplots(nrows=4, ncols=2, figsize=(12, 20))\n",
    "\n",
    "ax1.plot(fit1.history['accuracy'],'r', label ='Accuracy of training data')\n",
    "ax1.plot(fit1.history['val_accuracy'],'b--', label ='Accuracy of validation data')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_xlabel('epochs')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(fit1.history['loss'],'r', label ='Loss of training data')\n",
    "ax2.plot(fit1.history['val_loss'],'b--', label ='Loss of validation data')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_xlabel('epochs')\n",
    "ax2.legend()\n",
    "\n",
    "\n",
    "\n",
    "ax3.plot(fit2.history['accuracy'],'r', label ='Accuracy of training data')\n",
    "ax3.plot(fit2.history['val_accuracy'],'b--', label ='Accuracy of validation data')\n",
    "ax3.set_ylabel('Accuracy')\n",
    "ax3.set_xlabel('epochs')\n",
    "ax3.legend()\n",
    "\n",
    "ax4.plot(fit2.history['loss'],'r', label ='Loss of training data')\n",
    "ax4.plot(fit2.history['val_loss'],'b--', label ='Loss of validation data')\n",
    "ax4.set_ylabel('Loss')\n",
    "ax4.set_xlabel('epochs')\n",
    "ax4.legend()\n",
    "\n",
    "\n",
    "\n",
    "ax5.plot(fit3.history['accuracy'],'r', label ='Accuracy of training data')\n",
    "ax5.plot(fit3.history['val_accuracy'],'b--', label ='Accuracy of validation data')\n",
    "ax5.set_ylabel('Accuracy')\n",
    "ax5.set_xlabel('epochs')\n",
    "ax5.legend()\n",
    "\n",
    "ax6.plot(fit3.history['loss'],'r', label ='Loss of training data')\n",
    "ax6.plot(fit3.history['val_loss'],'b--', label ='Loss of validation data')\n",
    "ax6.set_ylabel('Loss')\n",
    "ax6.set_xlabel('epochs')\n",
    "ax6.legend()\n",
    "\n",
    "\n",
    "\n",
    "ax7.plot(fit4.history['accuracy'],'r', label ='Accuracy of training data')\n",
    "ax7.plot(fit4.history['val_accuracy'],'b--', label ='Accuracy of validation data')\n",
    "ax7.set_ylabel('Accuracy')\n",
    "ax7.set_xlabel('epochs')\n",
    "ax7.legend()\n",
    "\n",
    "ax8.plot(fit4.history['loss'],'r', label ='Loss of training data')\n",
    "ax8.plot(fit4.history['val_loss'],'b--', label ='Loss of validation data')\n",
    "ax8.set_ylabel('Loss')\n",
    "ax8.set_xlabel('epochs')\n",
    "ax8.legend()\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
