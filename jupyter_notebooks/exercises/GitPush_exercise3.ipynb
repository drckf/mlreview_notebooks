{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File importation\n",
    "str0=\"ts_L60_Z12_A500_DX50_bias5_N10000\"\n",
    "fnamex =\"DATA/x_\" + str0 + \".csv\"\n",
    "fnamey =\"DATA/y_\" + str0 + \".csv\"\n",
    "\n",
    "x = np.loadtxt(fnamex,delimiter=',',dtype=float)\n",
    "N = len(x)\n",
    "print('Length of x = ',N)\n",
    "\n",
    "\n",
    "\n",
    "categ = np.loadtxt(fnamey,delimiter=',', dtype=int)\n",
    "#Number of categories\n",
    "N_categ = 3\n",
    "y = np.zeros((N,N_categ))\n",
    "\n",
    "for n in range(N):\n",
    "    y[n][categ[n]] = 1\n",
    "    \n",
    "print(y[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We saw that the nn was not converging and it might be\n",
    "#due to some average, so we are going to remove it\n",
    "\n",
    "xm = x.mean(axis=1)\n",
    "for n in range(N):\n",
    "    x[n] = x[n]-xm[n]\n",
    "    \n",
    "std = x.std(axis=1)\n",
    "for n in range(N):\n",
    "    x[n] = x[n] / std[n]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data visualization\n",
    "plt.plot(x[0])\n",
    "plt.plot(x[1])\n",
    "plt.plot(x[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We asign the data for training and test\n",
    "perc_train = 0.8\n",
    "N_train = int(N * perc_train)\n",
    "N_val = N-N_train\n",
    "\n",
    "x_train = x[:N_train]\n",
    "y_train = y[:N_train]\n",
    "\n",
    "x_val = x[N_train:]\n",
    "y_val = y[N_train:]\n",
    "\n",
    "L =len(x[0])\n",
    "\n",
    "print('Number of samples',N)\n",
    "print('Training samples',N_train)\n",
    "print('Validation samples',N_val)\n",
    "print('Length of x[0]',L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0],L,1)\n",
    "x_val = x_val.reshape(x_val.shape[0],L,1)\n",
    "input_shape = (L,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import initializers,regularizers\n",
    "np.random.seed(12345)\n",
    "\n",
    "#We use l1 for Lazzo regularization\n",
    "reg = regularizers.l1(0.1)   \n",
    "\n",
    "#Now we initialize the NN weights\n",
    "ini = initializers.RandomNormal(mean=0,stddev=0.05)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters =5, kernel_size =11, \n",
    "                 kernel_regularizer = reg,\n",
    "                 kernel_initializer = ini,\n",
    "                 activation = 'relu',\n",
    "                 input_shape =input_shape))\n",
    "\n",
    "model.add(AveragePooling1D(5))\n",
    "model.add(Conv1D(filters=5,kernel_size=7,activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(N_categ,activation='softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "from keras import optimizers\n",
    "opt =optimizers.SGD(lr=0.01,momentum=0.9,nesterov=True,decay=1.e6)\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "             optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =250\n",
    "epochs = 30\n",
    "\n",
    "fit = model.fit(x_train,y_train,\n",
    "               batch_size=batch_size,\n",
    "               epochs = epochs,\n",
    "               validation_data =(x_val,y_val),\n",
    "               verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obs in ('accuracy','loss'):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(fit.history[obs],'r',\n",
    "            label =obs+' of training data')\n",
    "    \n",
    "    #dashed line!!\n",
    "    plt.plot(fit.history['val_'+obs],'b--',\n",
    "            label =obs+' of validation data')\n",
    "    \n",
    "    plt.ylabel(obs)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
